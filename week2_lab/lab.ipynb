{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3eeb53c",
   "metadata": {},
   "source": [
    "# Intro to Data Pipelines — Hands-On Lab for ML@P Accelerator Lab 2\n",
    "\n",
    "## Overview\n",
    "This lab is designed to give you practical experience with NumPy, Pandas, and Exploratory Data Analysis (EDA). \n",
    "\n",
    "The focus of this lab is not on writing perfect code, but on developing the ability to reason about data: its structure, quality, distributions, and relationships.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "- Create and manipulate NumPy arrays\n",
    "- Use vectorized operations instead of loops\n",
    "- Apply boolean masking and aggregation functions\n",
    "- Load and inspect tabular data using Pandas\n",
    "- Identify numerical and categorical columns\n",
    "- Handle missing values appropriately\n",
    "- Perform univariate and multivariate exploratory analysis\n",
    "- Interpret plots and summarize insights in words\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Lab Structure\n",
    "This lab is divided into two main parts:\n",
    "\n",
    "1. NumPy Exercises  \n",
    "\n",
    "\n",
    "2. Exploratory Data Analysis (EDA) with Pandas  \n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f5da64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918197be",
   "metadata": {},
   "source": [
    "## Part 1: NumPy Exercises\n",
    "**Tasks:**\n",
    "1. Create a 1D NumPy array containing the integers from 1 to 10 (inclusive)\n",
    "2. Create a 2D NumPy array of size (3, 3)\n",
    "3. For both arrays, print: ndim, shape, size and dtype\n",
    "\n",
    "4. Given\n",
    "arr = np.array([\n",
    "    [5, 10, 15],\n",
    "    [20, 25, 30],\n",
    "    [35, 40, 45]\n",
    "])\n",
    "\n",
    "    Extract the first row.\n",
    "    Extract the last column.\n",
    "    Extract the element with value 25.\n",
    "\n",
    "5. Using the array in part 1, filter for elements greater than 5\n",
    "6. Using the array in part 1, set all elements below 4 to 0\n",
    "7. Given data = np.array([10, 20, np.nan, 40, np.nan, 60])\n",
    "\n",
    "    Replace all NaNs with mean of non-nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dcc720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do your work here!\n",
    "\n",
    "# 1\n",
    "arr1 = np.array([1,2,3,4,5,6,7,8,9,10])\n",
    "# print(\"1:\", arr1)\n",
    "\n",
    "# 2\n",
    "arr2 = np.array([[1,2,3], [4,5,6], [7,8,9]])\n",
    "# print(\"2:\", arr2)\n",
    "\n",
    "# 3\n",
    "print(\"\\nQ3:\")\n",
    "\n",
    "print(arr1.ndim)\n",
    "print(arr1.shape)\n",
    "print(arr1.size)\n",
    "print(arr1.dtype)\n",
    "\n",
    "print()\n",
    "\n",
    "print(arr2.ndim)\n",
    "print(arr2.shape)\n",
    "print(arr2.size)\n",
    "print(arr2.dtype)\n",
    "\n",
    "# 4\n",
    "print(\"\\nQ4:\")\n",
    "\n",
    "arr3 = np.array([\n",
    "    [5, 10, 15],\n",
    "    [20, 25, 30],\n",
    "    [35, 40, 45]\n",
    "])\n",
    "\n",
    "print(arr3[0])\n",
    "print(arr3[:,0])\n",
    "print(arr3[1,1])\n",
    "\n",
    "# 5\n",
    "print(\"\\nQ5:\")\n",
    "\n",
    "print(arr1[arr1 > 5])\n",
    "\n",
    "# 6\n",
    "print(\"\\nQ6:\")\n",
    "\n",
    "arr4 = arr1.copy()\n",
    "arr4[arr4 < 4] = 0\n",
    "print(arr4)\n",
    "\n",
    "# 7\n",
    "print(\"\\nQ7:\")\n",
    "\n",
    "data = np.array([10, 20, np.nan, 40, np.nan, 60])\n",
    "data = np.where(np.isnan(data), np.nanmean(data), data)\n",
    "print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b86530",
   "metadata": {},
   "source": [
    "## Part 2: EDA\n",
    "\n",
    "For this part, we want you to explore a dataset of your choice by yourself! Head on over to Kaggle (https://www.kaggle.com/) to find a plethora of datasets in a plethora of fields. Try to find a dataset that will test skills taught in the workshop.\n",
    "\n",
    "Can't find something? Use the train Titanic dataset: https://www.kaggle.com/competitions/titanic/data?select=train.csv\n",
    "\n",
    "### Step 1 - Load the dataset using Pandas.\n",
    "\n",
    "**Tasks:**\n",
    "1. Read the CSV file into a DataFrame.\n",
    "2. Display the first 5 rows of the dataset.\n",
    "3. Print the shape of the DataFrame.\n",
    "4. Use info() to inspect column data types and missing values.\n",
    "5. Use describe() to view summary statistics for numerical columns.\n",
    "\n",
    "**Questions to answer:**\n",
    "- How many rows and columns does the dataset have?\n",
    "- Which columns are numerical?\n",
    "- Which columns are categorical?\n",
    "- Are there any obvious issues with data types or missing values?\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2 — Identify and Handle Missing Values\n",
    "\n",
    "**Tasks:**\n",
    "1. Identify how many missing values exist in each column.\n",
    "2. Decide which numerical columns should be imputed using:\n",
    "   - Mean, or\n",
    "   - Median, or\n",
    "   - Mode\n",
    "3. Perform the imputation.\n",
    "4. Verify that missing values have been handled correctly.\n",
    "\n",
    "**Questions to answer:**\n",
    "- Which columns contained missing values?\n",
    "- Why did you choose mean or median for each column?\n",
    "- Were any columns dropped? If yes, explain why.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3 — Univariate Analysis (Numerical Columns)\n",
    "\n",
    "Analyze numerical columns one at a time.\n",
    "\n",
    "**Tasks:**\n",
    "For at least two numerical columns:\n",
    "1. Plot a histogram.\n",
    "2. Plot a boxplot.\n",
    "3. Examine the summary statistics.\n",
    "\n",
    "**Questions to answer:**\n",
    "- Is the distribution skewed?\n",
    "- Are there extreme values or outliers?\n",
    "- Would scaling be required for this column before modeling?\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4 — Univariate Analysis (Categorical Columns)\n",
    "\n",
    "Analyze categorical columns individually.\n",
    "\n",
    "**Tasks:**\n",
    "1. Use value_counts() for each categorical column.\n",
    "2. Create a bar chart showing the distribution of categories.\n",
    "\n",
    "**Questions to answer:**\n",
    "- Which category appears most frequently?\n",
    "- Is there any noticeable class imbalance?\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5 — Feature Engineering \n",
    "\n",
    "Create at least one new feature that could be useful for analysis.\n",
    "\n",
    "**Questions to answer:**\n",
    "- Why might this feature be useful?\n",
    "- Is this new feature numerical or categorical?\n",
    "\n",
    "---\n",
    "\n",
    "### Step 6 — Multivariate Analysis\n",
    "\n",
    "Explore relationships between variables.\n",
    "\n",
    "#### Numerical vs Numerical\n",
    "**Tasks:**\n",
    "1. Create a scatter plot between 2 pairs of numerical variables\n",
    "2. Compute the correlation between these variables.\n",
    "\n",
    "**Questions to answer:**\n",
    "- Is there a visible relationship?\n",
    "- Is the correlation positive, negative or close to 0\n",
    "\n",
    "\n",
    "#### Categorical vs Numerical\n",
    "**Tasks:**\n",
    "1. Find statistics of numerical features after being grouped by the categorical features\n",
    "\n",
    "**Questions to answer:**\n",
    "- Are there any significant differences in the statistics across groups in the categorical variable?\n",
    "\n",
    "---\n",
    "\n",
    "### Step 7 — Outlier Analysis\n",
    "\n",
    "Focus on one numerical column with potential outliers.\n",
    "\n",
    "**Tasks:**\n",
    "1. Identify potential outliers using visual inspection (boxplot).\n",
    "2. Try to decide whether these outliers are:\n",
    "   - Data errors, or\n",
    "   - Legitimate extreme values\n",
    "\n",
    "**Questions to answer:**\n",
    "- Should these outliers be removed?\n",
    "- What is the IQR range, and upper, lower limits for removal?\n",
    "\n",
    "---\n",
    "\n",
    "### Step 8 — Final Insights\n",
    "\n",
    "Summarize your findings.\n",
    "\n",
    "**Tasks:**\n",
    "1. Write at least three meaningful insights derived from your analysis.\n",
    "2. Reference plots or statistics where appropriate.\n",
    "\n",
    "Examples of insights:\n",
    "- Patterns between variables\n",
    "- Differences across categories\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e366884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do your work here!\n",
    "\n",
    "### ! Step 1 - Load the dataset using Pandas.\n",
    "\n",
    "# **Tasks:**\n",
    "\n",
    "# ! THIS IS MY DATASET: https://www.kaggle.com/datasets/ahmedmohamed2003/cafe-sales-dirty-data-for-cleaning-training\n",
    "\n",
    "# TODO 1. Read the CSV file into a DataFrame.\n",
    "data = pd.read_csv('dirty_cafe_sales.csv')\n",
    "\n",
    "# TODO 2. Display the first 5 rows of the dataset.\n",
    "print(data.head(5))\n",
    "\n",
    "# TODO 3. Print the shape of the DataFrame.\n",
    "print(data.shape)\n",
    "\n",
    "# TODO 4. Use info() to inspect column data types and missing values.\n",
    "data.info()\n",
    "\n",
    "# TODO 5. Use describe() to view summary statistics for numerical columns.\n",
    "data.describe()\n",
    "\n",
    "# **Questions to answer:**\n",
    "# ?- How many rows and columns does the dataset have?\n",
    "    # ^ rows = 10,000\n",
    "    # ^ cols = 8\n",
    "\n",
    "# ?- Which columns are numerical?\n",
    "# ?- Which columns are categorical?\n",
    "    # ^ Types of data for the columns, Transaction ID: Numerical, Item: Categorical, Quantity: Numerical, Price Per Unit: Numerical\n",
    "    # ^ Total Spent: Numerical, Payment Method: Categorical, Location: Categorical, Transaction Date: DateTime\n",
    "\n",
    "# ?- Are there any obvious issues with data types or missing values?\n",
    "    # ^ There are missing values, values that have different ways of representing that they don't exist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e916f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### ! Step 2 — Identify and Handle Missing Values\n",
    "\n",
    "# **Tasks:**\n",
    "\n",
    "# TODO 1. Identify how many missing values exist in each column.\n",
    "data.isna().sum()\n",
    "\n",
    "# TODO 2. Decide which numerical columns should be imputed using:\n",
    "# TODO   - Mean, or\n",
    "# TODO   - Median, or\n",
    "# TODO   - Mode\n",
    "\n",
    "# ^ Quantity: Numerical - mean by item\n",
    "# ^ Price Per Unit: Numerical - mean by item\n",
    "# ^ Total Spent: Numerical - quantity * price per unit\n",
    "\n",
    "# TODO 3. Perform the imputation.\n",
    "# ~ Quantity\n",
    "# data['Quantity'].unique()\n",
    "\n",
    "# errors = 'coerce' will convert non-numeric values to NaN, the line above is not really necessary but makes it cleaner\n",
    "data['Quantity'] = data['Quantity'].replace(['ERROR', 'UNKNOWN'], np.nan)\n",
    "data['Quantity'] = pd.to_numeric(data['Quantity'], errors='coerce')\n",
    "data['Quantity'] = data['Quantity'].fillna(data.groupby('Item')['Quantity'].transform('mean'))\n",
    "data['Quantity'] = data['Quantity'].round()\n",
    "\n",
    "# data['Quantity'].unique()\n",
    "\n",
    "# ~ Price Per Unit\n",
    "# data['Price Per Unit'].unique()\n",
    "\n",
    "data['Price Per Unit'] = data['Price Per Unit'].replace(['ERROR', 'UNKNOWN'], np.nan)\n",
    "data['Price Per Unit'] = pd.to_numeric(data['Price Per Unit'], errors='coerce')\n",
    "data['Price Per Unit'] = data['Price Per Unit'].fillna(data.groupby('Item')['Price Per Unit'].transform('mean'))\n",
    "data['Price Per Unit'] = data['Price Per Unit'].round()\n",
    "\n",
    "# data['Price Per Unit'].unique()\n",
    "# data[data['Price Per Unit'] < 1]\n",
    "# data[(data['Quantity' * data['Price Per Unit'] != data['Total Spent']]]\n",
    "\n",
    "# ~ Total Spent - even though there are actual values for most of them, this will fill in the missing ones correctly and change everything to float\n",
    "data['Total Spent'] = data['Quantity'] * data['Price Per Unit']\n",
    "\n",
    "# ~ Remove rows with missing Payment Method or Location or Transaction Date and remove ID column, but I will keep an old version of the data for all the numerical values\n",
    "data[['Item', 'Payment Method', 'Location', 'Transaction Date']] = data[['Item','Payment Method', 'Location', 'Transaction Date']].replace(['ERROR', 'UNKNOWN'], np.nan)\n",
    "\n",
    "data_values = data.dropna(subset=['Quantity', 'Price Per Unit', 'Total Spent']).drop(columns=['Transaction ID']).reset_index(drop = True)\n",
    "\n",
    "data_cleaned = data.dropna(subset=['Item', 'Payment Method', 'Location', 'Transaction Date']).drop(columns=['Transaction ID']).reset_index(drop = True)\n",
    "\n",
    "# len(data_cleaned)\n",
    "\n",
    "# TODO 4. Verify that missing values have been handled correctly.\n",
    "\n",
    "# data_values['Quantity'].unique()\n",
    "# data_values[data_values['Quantity'].isna()]\n",
    "# data_values['Price Per Unit'].unique()\n",
    "\n",
    "# data_values.head(20)\n",
    "# data_cleaned.head(20)\n",
    "\n",
    "# ^ only the numerical values have no missing values, this is for full numerical analysis\n",
    "# ^ later on, we can choose to drop rows for specific categorical data analysis if needed, but this allows us to retain as much NECESSARY data as we can\n",
    "print(data_values.isna().sum())\n",
    "# Item                 929\n",
    "# Quantity               0\n",
    "# Price Per Unit         0\n",
    "# Total Spent            0\n",
    "# Payment Method      3170\n",
    "# Location            3944\n",
    "# Transaction Date     459\n",
    "# dtype: int64\n",
    "\n",
    "# ^ fully cleaned data with no missing values\n",
    "print(data_cleaned.isna().sum())\n",
    "# Item                0\n",
    "# Quantity            0\n",
    "# Price Per Unit      0\n",
    "# Total Spent         0\n",
    "# Payment Method      0\n",
    "# Location            0\n",
    "# Transaction Date    0\n",
    "# dtype: int64\n",
    "\n",
    "# **Questions to answer:**\n",
    "# ?- Which columns contained missing values?\n",
    "# ^ all but the id column\n",
    "\n",
    "# ?- Why did you choose mean or median for each column?\n",
    "# ^ I chose the mean for all the numerical columns based on item because it will just give the correct value since it wouldn't change, or a good approximation\n",
    "# ^ I then rounded the values so they matched properly\n",
    "\n",
    "# ?- Were any columns dropped? If yes, explain why.\n",
    "# ^ I dropped the ID column, and I also dropped any values that were missing Type, Payment, Location, or Date since you can't really impute those \n",
    "# ^ values and they are important for analysis. However, I kept another version that has all of the data that can be narrowed down for specific analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46565073",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### ! Step 3 — Univariate Analysis (Numerical Columns)\n",
    "\n",
    "# ! Analyze numerical columns one at a time.\n",
    "\n",
    "# **Tasks:**\n",
    "# TODO For at least two numerical columns:\n",
    "\n",
    "# TODO 1 & 2. Plot a histogram and boxplot side by side for 'Total Spent'\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram for 'Total Spent'\n",
    "axes[0].hist(data_values['Total Spent'], bins=[0, 4, 8, 12, 16, 20], color='skyblue', edgecolor='black', rwidth=0.9)\n",
    "axes[0].set_title('Distribution of Total Spent')\n",
    "axes[0].set_xlabel('Total Spent ($)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_xticks([0, 4, 8, 12, 16, 20])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Boxplot for 'Total Spent'\n",
    "axes[1].boxplot(data_values['Total Spent'], patch_artist=True, boxprops=dict(facecolor='skyblue'))\n",
    "axes[1].set_title('Boxplot of Total Spent')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Histogram and boxplot for 'Price Per Unit'\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(data_values['Price Per Unit'], bins=range(int(data_values['Price Per Unit'].min()), int(data_values['Price Per Unit'].max()) + 2), color='lightgreen', edgecolor='black', rwidth=0.9)\n",
    "axes[0].set_title('Distribution of Price Per Unit')\n",
    "axes[0].set_xlabel('Price Per Unit ($)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "axes[1].boxplot(data_values['Price Per Unit'], patch_artist=True, boxprops=dict(facecolor='lightgreen'))\n",
    "axes[1].set_title('Boxplot of Price Per Unit')\n",
    "axes[1].set_ylabel('Price Per Unit ($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# TODO 3. Examine the summary statistics.\n",
    "\n",
    "print(data_values[['Total Spent', 'Price Per Unit']].describe())\n",
    "#        Total Spent  Price Per Unit\n",
    "# count  9960.000000     9960.000000\n",
    "# mean      9.113855        3.007831\n",
    "# std       5.837844        1.219770\n",
    "# min       1.000000        1.000000\n",
    "# 25%       4.000000        2.000000\n",
    "# 50%       8.000000        3.000000\n",
    "# 75%      12.000000        4.000000\n",
    "# max      25.000000        5.000000\n",
    "\n",
    "# **Questions to answer:**\n",
    "# ? - Is the distribution skewed?\n",
    "# ^ Total Spent is right skewed, Price Per Unit is normal\n",
    "\n",
    "# ? - Are there extreme values or outliers?\n",
    "# ^ Total Spent has some outliers, Price Per Unit has some extreme values, but no outliers (it's range is much smaller and it's data seems more consistent)\n",
    "# ^ the outliers for Total Spent are only a little above the upper quartile, they are at 25 and the limit is 24 (Q3 + 1.5*IQR = 12 + 1.5*8 = 24)\n",
    "\n",
    "# ? - Would scaling be required for this column before modeling?\n",
    "# ^ Scaling may be needed for Total Spent because of its wider range, outliers, and right skew, \n",
    "# ^ Price Per Unit probably doesn't need scaling since it's so consistent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994ed94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### ! Step 4 — Univariate Analysis (Categorical Columns\n",
    "\n",
    "# ! Analyze categorical columns individually\n",
    "\n",
    "# **Tasks:**\n",
    "# TODO 1. Use value_counts() for each categorical column.\n",
    "# print(data_cleaned[\"Item\"].value_counts())\n",
    "# Item\n",
    "# Salad       481\n",
    "# Juice       481\n",
    "# Cookie      465\n",
    "# Sandwich    461\n",
    "# Cake        442\n",
    "# Tea         429\n",
    "# Coffee      428\n",
    "# Smoothie    393\n",
    "# Name: count, dtype: int6\n",
    "\n",
    "# print(data_cleaned[\"Payment Method\"].value_counts())\n",
    "# Payment Method\n",
    "# Digital Wallet    1245\n",
    "# Cash              1170\n",
    "# Credit Card       1165\n",
    "# Name: count, dtype: int6\n",
    "\n",
    "# print(data_cleaned[\"Location\"].value_counts())\n",
    "# Location\n",
    "# Takeaway    1797\n",
    "# In-store    1783\n",
    "# Name: count, dtype: int6\n",
    "\n",
    "# TODO 2. Create a bar chart showing the distribution of categories\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].bar(data_cleaned['Item'].value_counts().index, data_cleaned['Item'].value_counts().values, color='lightcoral', edgecolor='black')\n",
    "axes[0].set_title('Distribution of Items')\n",
    "axes[0].set_xticklabels(data_cleaned['Item'].value_counts().index, rotation=30)\n",
    "\n",
    "axes[1].bar(data_cleaned['Payment Method'].value_counts().index, data_cleaned['Payment Method'].value_counts().values, color='lightblue', edgecolor='black')\n",
    "axes[1].set_title('Distribution of Payment Methods')\n",
    "\n",
    "axes[2].bar(data_cleaned['Location'].value_counts().index, data_cleaned['Location'].value_counts().values, color='lightgreen', edgecolor='black')\n",
    "axes[2].set_title('Distribution of Locations')\n",
    "\n",
    "\n",
    "# **Questions to answer:**\n",
    "# ? - Which category appears most frequently?\n",
    "# ^ Item: Salad and Juice\n",
    "# ^ Payment Method: Digital Wallet\n",
    "# ^ Location: Takeaway\n",
    "\n",
    "\n",
    "# ? - Is there any noticeable class imbalance?\n",
    "# ^ Not really, it's pretty balanced, there is less than a 100 difference in the \"Distribution of Items\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1429c3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### ! Step 5 — Feature Engineering \n",
    "\n",
    "# TODO Create at least one new feature that could be useful for analysis.\n",
    "data_dates = data_values.loc[data_values['Transaction Date'].notna()]\n",
    "\n",
    "transactionMonth = pd.to_datetime(data_dates['Transaction Date']).dt.month\n",
    "transactionMonth = transactionMonth.map({1: 'January', 2: 'February', 3: 'March'\n",
    "                                         , 4: 'April', 5: 'May', 6: 'June'\n",
    "                                         , 7: 'July', 8: 'August', 9: 'September'\n",
    "                                         , 10: 'October', 11: 'November', 12: 'December'})\n",
    "\n",
    "data_dates['Transaction Month'] = transactionMonth\n",
    "\n",
    "# **Questions to answer:**\n",
    "# ? - Why might this feature be useful?\n",
    "# ^ I thought we could see some trends between spending and time of year. This could reveal seasonal or holiday biases.\n",
    "\n",
    "# ? - Is this new feature numerical or categorical?\n",
    "# ^ Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e863022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### ! Step 6 — Multivariate Analysis\n",
    "\n",
    "# ! Explore relationships between variables.\n",
    "\n",
    "#### ~ Numerical vs Numerical\n",
    "# **Tasks:**\n",
    "# TODO 1. Create a scatter plot between 2 pairs of numerical variables\n",
    "ax = data_values.plot.scatter(x='Price Per Unit', y='Total Spent', alpha=0.5, color='orchid', edgecolor='black')\n",
    "plt.title('Scatter Plot of Price Per Unit vs Total Spent')\n",
    "plt.xlabel('Price Per Unit ($)')\n",
    "plt.ylabel('Total Spent ($)')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# TODO 2. Compute the correlation between these variables.\n",
    "correlation = data_values['Price Per Unit'].corr(data_values['Total Spent'])\n",
    "print(f'Correlation between Price Per Unit and Total Spent: {correlation:.2f}')\n",
    "# Correlation between Price Per Unit and Total Spent: 0.63\n",
    "\n",
    "# **Questions to answer:**\n",
    "# ? - Is there a visible relationship?\n",
    "# ^ No, these values are at exact points so a scatter plot does not help at all\n",
    "\n",
    "# ? - Is the correlation positive, negative or close to 0\n",
    "# ^ Positive correlation of 0.63\n",
    "\n",
    "\n",
    "#### ~ Categorical vs Numerical\n",
    "# **Tasks:**\n",
    "# TODO 1. Find statistics of numerical features after being grouped by the categorical features\n",
    "\n",
    "# ~ This is using the new feature created in Step 5\n",
    "plt.figure(figsize=(10, 6))\n",
    "data_dates.groupby('Transaction Month')['Total Spent'].mean().reindex(['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']).plot(kind='bar', color='mediumseagreen', edgecolor='black')\n",
    "plt.title('Average Total Spent by Transaction Month')\n",
    "plt.xlabel('Transaction Month')\n",
    "plt.ylabel('Average Total Spent ($)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# **Questions to answer:**\n",
    "# ? - Are there any significant differences in the statistics across groups in the categorical variable?\n",
    "# ^ Not really, the average spend across all months is very similar, all around $9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391d80cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### ! Step 7 — Outlier Analysis\n",
    "\n",
    "# ! Focus on one numerical column with potential outliers.\n",
    "\n",
    "# **Tasks:**\n",
    "# TODO 1. Identify potential outliers using visual inspection (boxplot).\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "ax.boxplot(data_values['Total Spent'], patch_artist=True, boxprops=dict(facecolor='skyblue'))\n",
    "ax.set_title('Boxplot of Total Spent')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# TODO 2. Try to decide whether these outliers are:\n",
    "# TODO   - Data errors, or\n",
    "# ^ They are NOT data errors, they are valid values for Total Spent\n",
    "\n",
    "# TODO   - Legitimate extreme values\n",
    "# ^ Yes, but barely because the cutoff is at 24 and the max is 25\n",
    "\n",
    "# **Questions to answer:**\n",
    "# ? - Should these outliers be removed?\n",
    "# ^ No, they are valid values\n",
    "\n",
    "# ? - What is the IQR range, and upper, lower limits for removal?\n",
    "# ^ IQR = 8, Lower limit = 0, Upper limit = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16a345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### ! Step 8 — Final Insights\n",
    "\n",
    "# ! Summarize your findings.\n",
    "\n",
    "# **Tasks:**\n",
    "# TODO 1. Write at least three meaningful insights derived from your analysis.\n",
    "# TODO 2. Reference plots or statistics where appropriate.\n",
    "# ^ 1. The most popular items are Salad and Juice, while the least popular is Smoothie\n",
    "    # ~ This is shown in the \"Distribution of Items\" bar chart in Step 4\n",
    "\n",
    "# ^ 2. The average Total Spent is around $9, no matter the time of year\n",
    "    # ~ This is shown in the \"Average Total Spent by Transaction Month\" bar chart in Step 6\n",
    "\n",
    "# ^ 3. There is a moderate positive correlation (0.63) between Price Per Unit and Total Spent\n",
    "    # ~ This is shown in the scatter plot and correlation value in Step 6\n",
    "\n",
    "# ! DISCLAMER: THIS DATA IS SYNTHETIC WHICH IS WHY THERE ISN'T VERY MEANINGFUL INSIGHTS (it was great for EDA, though)\n",
    "\n",
    "# ~ Examples of insights:\n",
    "# ~ - Patterns between variables\n",
    "# ~ - Differences across categories"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
