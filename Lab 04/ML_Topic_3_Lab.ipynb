{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Lab 3: Regularized Regression Pipeline\n",
        "\n",
        "Dataset: Ames Housing (House Prices)\n",
        "\n",
        "Goal: Predict the SalePrice of a home using physical attributes while preventing overfitting through regularization."
      ],
      "metadata": {
        "id": "CUvTvfe9f5Bp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Objective\n",
        "\n",
        "Build an end-to-end regression pipeline. Make sure your model handles numerical data, encode neighborhood information, and use Cross-Validation to decide which regularization method (Lasso or Ridge) is better for predicting home values. Compare it against a non-regularized model, what performs better?"
      ],
      "metadata": {
        "id": "WCmOTQvHgBnc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Key Concepts and Terms\n",
        "\n",
        "**High Cardinality**: The `Neighborhood` column has many unique values. We must encode this carefully.\n",
        "\n",
        "**The Alpha ($\\alpha$) Parameter**: In Scikit-Learn's Ridge and Lasso, $\\alpha$ is the regularization strength.\n",
        "\n",
        "$\\alpha = 0$: Standard Linear Regression.\n",
        "\n",
        "$\\alpha > 0$: Increasing penalty on model complexity.\n",
        "\n",
        "**Data Leakage:** A critical error where information from the test set (like the mean house price) \"leaks\" into the training process."
      ],
      "metadata": {
        "id": "9QZdUz3cgXZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Necessary imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.datasets import fetch_openml"
      ],
      "metadata": {
        "id": "5iLJ2fg3gxZC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1: Data Preparation**\n",
        "\n",
        "Load the dataset and separate the target (`SalePrice`) from the features. Perform an 80/20 split."
      ],
      "metadata": {
        "id": "mLjTMn19g0sD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing = fetch_openml(name=\"house_prices\", as_frame=True, parser='auto')\n",
        "X = housing.data[['GrLivArea', 'LotArea', 'OverallQual', 'Neighborhood', 'HouseStyle']]\n",
        "y = housing.target\n",
        "\n",
        "# TODO: Split the data (80/20 split, random_state=42)"
      ],
      "metadata": {
        "id": "PsAOD0OtgyON"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2: The Preprocessing Recipe**\n",
        "\n",
        "Create a `ColumnTransformer` that applies different logic to different columns:\n",
        "\n",
        "Numeric Features (`GrLivArea`, `LotArea`): These have huge scales. Apply `StandardScaler`.\n",
        "\n",
        "Categorical Features (`Neighborhood`, `HouseStyle`): Use `OneHotEncoder`. Use `handle_unknown='ignore'` because a test set might have a neighborhood the training set didn't see."
      ],
      "metadata": {
        "id": "ldW6fJ9dhGix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_features = ['GrLivArea', 'LotArea', 'OverallQual']\n",
        "categorical_features = ['Neighborhood', 'HouseStyle']\n",
        "\n",
        "# TODO: Create a numeric transformer (Imputer + Scaler)\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', # YOUR CODE HERE)\n",
        "])\n",
        "\n",
        "# TODO: Create a categorical transformer (Imputer + OneHotEncoder)\n",
        "# Hint: remember drop='first' to avoid the dummy variable trap!\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', # YOUR CODE HERE)\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "2fUaIw0Fhkbc",
        "outputId": "c6f64510-38ee-4d8e-c0ef-82dabfc347be"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "closing parenthesis ']' does not match opening parenthesis '(' on line 7 (ipython-input-804891352.py, line 8)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-804891352.py\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    ])\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m closing parenthesis ']' does not match opening parenthesis '(' on line 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3: Pipeline**\n",
        "\n",
        "Construct a Pipeline that connects your **preprocessor** to a regressor. You will test both **Ridge()** and **Lasso().**"
      ],
      "metadata": {
        "id": "RuWktZUch0R7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# TODO: Create a pipeline that joins the preprocessor with Ridge regression\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', ),\n",
        "    ('regressor', )\n",
        "])"
      ],
      "metadata": {
        "id": "gxeEYqlxhuw4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 4: Hyperparameter Tuning**\n",
        "\n",
        "Use `GridSearchCV` to find the best `alpha` for your model. Test values across several orders of magnitude (e.g., 0.1, 1, 10, 100)."
      ],
      "metadata": {
        "id": "95_uhsK2iVF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    # YOUR CODE HERE\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2')\n",
        "grid_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "TgDBoR3AiXLk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 5: Evaluation**\n",
        "\n",
        "Print out the best params for `alpha` and the corresponding R^2 score."
      ],
      "metadata": {
        "id": "UH-8LsB3ioFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best Alpha: {grid_search.best_params_}\")\n",
        "print(f\"Test R^2 Score: {grid_search.score(X_test, y_test):.4f}\")"
      ],
      "metadata": {
        "id": "QEblaZcKi3Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Questions to Consider**\n",
        "\n",
        "\n",
        "\n",
        "*   Compare the results if we change the regressor to `Lasso()`, Does the R^@ score improve?\n",
        "*   Lasso can set coefficients to zero, why might that be good for datasets like this with 70+ features?\n",
        "*   If we ended up not using `Pipeline` and scaling our data before the train-test split, what implications does this have on the model's accuracy score?\n",
        "\n"
      ],
      "metadata": {
        "id": "exgq8T1-i8HM"
      }
    }
  ]
}