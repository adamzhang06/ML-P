{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eah2ptRCG-Jv"
      },
      "source": [
        "# **Build & Train a 2-Layer Neural Network on Tabular Data ~ PyTorch**"
      ],
      "id": "Eah2ptRCG-Jv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRpiA-jDG-Jx"
      },
      "source": [
        "### **Outline**\n",
        "1. Setup and reproducibility\n",
        "2. Create a synthetic tabular dataset\n",
        "3. Train/test split + `DataLoader`\n",
        "4. Define a 2-layer network\n",
        "5. Train and track metrics\n",
        "6. Evaluate the final model\n",
        "7. Exercise + common pitfalls + extension"
      ],
      "id": "DRpiA-jDG-Jx"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZ-AUAr1G-Jx",
        "outputId": "83ec05c5-c791-457b-ac77-c08ed9b1edfb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# If torch is missing, install it in a notebook cell:\n",
        "# %pip install torch\n",
        "\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# CPU is enough for this lab\n",
        "device = torch.device(\"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "id": "RZ-AUAr1G-Jx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpL1z3mQG-Jy"
      },
      "source": [
        "## Step 1 - Create Tabular Data\n",
        "We will generate a small binary-classification dataset with 10 numeric features.\n",
        "The target is based on a nonlinear rule so the hidden layer has a purpose."
      ],
      "id": "WpL1z3mQG-Jy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWu2AYAiG-Jy",
        "outputId": "090addcf-0785-4058-9600-2b41f474c86f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: torch.Size([1000, 10])\n",
            "y shape: torch.Size([1000, 1])\n",
            "Positive class rate: 0.39500001072883606\n"
          ]
        }
      ],
      "source": [
        "num_samples = 1000\n",
        "num_features = 10\n",
        "\n",
        "X = torch.randn(num_samples, num_features)\n",
        "\n",
        "# Nonlinear signal + noise\n",
        "signal = (\n",
        "    1.4 * X[:, 0]\n",
        "    - 1.0 * X[:, 1]\n",
        "    + 0.8 * X[:, 2] * X[:, 3]\n",
        "    - 0.5 * X[:, 4] ** 2\n",
        ")\n",
        "noise = 0.25 * torch.randn(num_samples)\n",
        "logits_true = signal + noise\n",
        "\n",
        "probs_true = torch.sigmoid(logits_true)\n",
        "y = (probs_true > 0.5).float().unsqueeze(1)\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "print(\"Positive class rate:\", y.mean().item())"
      ],
      "id": "AWu2AYAiG-Jy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huQ_3XvpG-Jy"
      },
      "source": [
        "## Step 2 - Split Data and Build DataLoaders\n",
        "We will keep 80% for training and 20% for testing.\n",
        "`DataLoader` gives us shuffled mini-batches for training."
      ],
      "id": "huQ_3XvpG-Jy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdBDU9TyG-Jy",
        "outputId": "4674546a-0ca9-4e8e-cff8-e5fdd8414a66"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 25\n",
            "Test batches: 4\n"
          ]
        }
      ],
      "source": [
        "train_ratio = 0.8\n",
        "train_size = int(num_samples * train_ratio)\n",
        "\n",
        "indices = torch.randperm(num_samples)\n",
        "train_idx = indices[:train_size]\n",
        "test_idx = indices[train_size:]\n",
        "\n",
        "X_train, y_train = X[train_idx], y[train_idx]\n",
        "X_test, y_test = X[test_idx], y[test_idx]\n",
        "\n",
        "train_ds = TensorDataset(X_train, y_train)\n",
        "test_ds = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "print(\"Train batches:\", len(train_loader))\n",
        "print(\"Test batches:\", len(test_loader))"
      ],
      "id": "AdBDU9TyG-Jy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8ibpdPKG-Jz"
      },
      "source": [
        "## Step 3 - Define a 2-Layer Neural Network\n",
        "Architecture:\n",
        "- Layer 1: `Linear(input_dim -> hidden_dim)`\n",
        "- Activation: `ReLU`\n",
        "- Layer 2: `Linear(hidden_dim -> 1)` (outputs logits)"
      ],
      "id": "S8ibpdPKG-Jz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbwLTGIaG-Jz",
        "outputId": "164c6f2e-433c-4e3d-fffd-b9fb2f2da2f1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TwoLayerNN(\n",
            "  (net): Sequential(\n",
            "    (0): Linear(in_features=10, out_features=32, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
            "  )\n",
            ")\n",
            "Trainable parameters: 385\n"
          ]
        }
      ],
      "source": [
        "class TwoLayerNN(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int = 32):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = TwoLayerNN(input_dim=num_features, hidden_dim=32).to(device)\n",
        "\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print(model)\n",
        "print(\"Trainable parameters:\", num_params)"
      ],
      "id": "gbwLTGIaG-Jz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiwZLwtMG-Jz"
      },
      "source": [
        "## Step 4 - Loss, Optimizer, and Helper Functions\n",
        "For binary classification with logits, use `BCEWithLogitsLoss`.\n",
        "This is more numerically stable than `sigmoid` + `BCELoss`."
      ],
      "id": "BiwZLwtMG-Jz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJicLtMZG-Jz"
      },
      "execution_count": 5,
      "outputs": [],
      "source": [
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "\n",
        "def batch_accuracy_from_logits(logits, targets):\n",
        "    preds = (torch.sigmoid(logits) >= 0.5).float()\n",
        "    return (preds == targets).float().mean().item()\n",
        "\n",
        "def run_epoch(model, loader, loss_fn, optimizer=None):\n",
        "    training = optimizer is not None\n",
        "    model.train() if training else model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_acc = 0.0\n",
        "    total_items = 0\n",
        "\n",
        "    with torch.set_grad_enabled(training):\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "\n",
        "            logits = model(xb)\n",
        "            loss = loss_fn(logits, yb)\n",
        "\n",
        "            if training:\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            bs = xb.size(0)\n",
        "            total_loss += loss.item() * bs\n",
        "            total_acc += batch_accuracy_from_logits(logits, yb) * bs\n",
        "            total_items += bs\n",
        "\n",
        "    return total_loss / total_items, total_acc / total_items"
      ],
      "id": "lJicLtMZG-Jz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4AuA_CGG-J0"
      },
      "source": [
        "## Step 5 - Train the Model\n",
        "We will train for a small number of epochs and print train/test metrics."
      ],
      "id": "S4AuA_CGG-J0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrTA7faFG-J0",
        "outputId": "410b7b5f-9222-43c0-cbd9-89aa08c1c423"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | train_loss=0.6211 train_acc=0.625 | test_loss=0.4771 test_acc=0.860\n",
            "Epoch 05 | train_loss=0.2043 train_acc=0.919 | test_loss=0.2157 test_acc=0.920\n",
            "Epoch 10 | train_loss=0.1162 train_acc=0.960 | test_loss=0.1504 test_acc=0.950\n",
            "Epoch 15 | train_loss=0.0808 train_acc=0.978 | test_loss=0.1893 test_acc=0.930\n",
            "Epoch 20 | train_loss=0.0635 train_acc=0.980 | test_loss=0.1571 test_acc=0.945\n"
          ]
        }
      ],
      "source": [
        "epochs = 20\n",
        "history = []\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss, train_acc = run_epoch(model, train_loader, loss_fn, optimizer)\n",
        "    test_loss, test_acc = run_epoch(model, test_loader, loss_fn, optimizer=None)\n",
        "\n",
        "    history.append((train_loss, train_acc, test_loss, test_acc))\n",
        "\n",
        "    if epoch == 1 or epoch % 5 == 0:\n",
        "        print(\n",
        "            f\"Epoch {epoch:02d} | \"\n",
        "            f\"train_loss={train_loss:.4f} train_acc={train_acc:.3f} | \"\n",
        "            f\"test_loss={test_loss:.4f} test_acc={test_acc:.3f}\"\n",
        "        )"
      ],
      "id": "JrTA7faFG-J0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SfP78tuG-J1"
      },
      "source": [
        "## Step 6 - Final Evaluation and Quick Interpretation\n",
        "We will inspect test accuracy and a few predicted probabilities."
      ],
      "id": "2SfP78tuG-J1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXJc3DCQG-J1",
        "outputId": "a1fb8945-413f-4e87-b3ab-bfa2a47377fa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final train accuracy: 0.980\n",
            "Final test accuracy:  0.945\n",
            "First 10 predicted probabilities:\n",
            "tensor([2.3238e-05, 7.3397e-01, 6.5447e-01, 1.5740e-01, 5.6465e-08, 9.9960e-01,\n",
            "        9.9734e-01, 7.7261e-09, 1.8203e-03, 8.1115e-01])\n",
            "First 10 true labels:\n",
            "tensor([0., 1., 1., 0., 0., 1., 1., 0., 0., 1.])\n"
          ]
        }
      ],
      "source": [
        "final_train_loss, final_train_acc = history[-1][0], history[-1][1]\n",
        "final_test_loss, final_test_acc = history[-1][2], history[-1][3]\n",
        "\n",
        "print(f\"Final train accuracy: {final_train_acc:.3f}\")\n",
        "print(f\"Final test accuracy:  {final_test_acc:.3f}\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    sample_logits = model(X_test[:10].to(device))\n",
        "    sample_probs = torch.sigmoid(sample_logits).squeeze(1).cpu()\n",
        "\n",
        "print(\"First 10 predicted probabilities:\")\n",
        "print(sample_probs)\n",
        "print(\"First 10 true labels:\")\n",
        "print(y_test[:10].squeeze(1))"
      ],
      "id": "FXJc3DCQG-J1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ndsF7UQG-J2"
      },
      "source": [
        "## Exercise\n",
        "Try improving the model in one controlled change at a time.\n",
        "\n",
        "Ideas:\n",
        "1. Change hidden units from 32 to 64\n",
        "2. Lower learning rate from `1e-2` to `1e-3`\n",
        "3. Train for 40 epochs\n",
        "\n",
        "Record your best test accuracy and explain what changed."
      ],
      "id": "3ndsF7UQG-J2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypUHz0A2G-J2"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: Copy the training section and run one change at a time.\n",
        "# Suggested template:\n",
        "# - hidden_dim = ?\n",
        "# - learning_rate = ?\n",
        "# - epochs = ?\n",
        "# - best_test_acc = ?\n",
        "# - short explanation = \"...\""
      ],
      "id": "ypUHz0A2G-J2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD-U89B5G-J2"
      },
      "source": [
        "## Common Pitfall\n",
        "Applying `sigmoid` inside the model **and** using `BCEWithLogitsLoss`.\n",
        "\n",
        "Why it is a problem:\n",
        "- `BCEWithLogitsLoss` already applies sigmoid internally.\n",
        "- Double sigmoid can hurt gradients and slow learning.\n",
        "\n",
        "## Optional Extension\n",
        "Turn this into a multiclass task by:\n",
        "- Changing the final layer to output `num_classes`\n",
        "- Using `CrossEntropyLoss`\n",
        "- Predicting class with `argmax`"
      ],
      "id": "CD-U89B5G-J2"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}